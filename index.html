<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Antoni Bigata Casademunt</title>

    <meta name="author" content="Antoni Bigata Casademunt">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/dragonball.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

    
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Antoni Bigata Casademunt
                </p>
                <p>PhD student at Imperial College London. Working on human avatars and generative AI. Intern @ Meta and @ Disney Research</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:ab4522@ic.ac.uk">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=LuIdiV8AAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/toninio444/">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/antonibigata">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/antoni-bigata/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about human face/head generation and manipu.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="keyface_stop()" onmouseover="keyface_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='keyface_image'><video width=100% muted autoplay loop>
          <source src="images/front_row_1_grid.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/keyface_icon.png' width=100%>
        </div>
        <script type="text/javascript">
          function keyface_start() {
            document.getElementById('keyface_image').style.opacity = "1";
          }

          function keyface_stop() {
            document.getElementById('keyface_image').style.opacity = "0";
          }
          keyface_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://antonibigata.github.io/KeyFace/">
          <span class="papertitle">KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation</span>
        </a>
        <br>
        <strong>Antoni Bigata</strong>,
        <a href="https://scholar.google.com/citations?user=ty2OYvcAAAAJ">Micha≈Ç Stypu≈Çkowski</a>,
        <a href="https://scholar.google.com/citations?user=08YfKjcAAAAJ">Rodrigo Mira</a>,
        <a href="https://scholar.google.com/citations?user=zdg4dj0AAAAJ">Stella Bounareli</a>,
        <a href="https://scholar.google.com/citations?user=WwLpK44AAAAJ">Konstantinos Vougioukas</a>,
        <a href="https://scholar.google.com/citations?user=46APmkYAAAAJ">Zoe Landgraf</a>,
        <a href="https://scholar.google.com/citations?user=itNst7wAAAAJ">Nikita Drobyshev</a>,
        <a href="https://scholar.google.com/citations?user=XmOBJZYAAAAJ">Maciej Zieba</a>,
        <a href="https://scholar.google.com/citations?user=6v-UKEMAAAAJ">Stavros Petridis</a>,
        <a href="https://scholar.google.com/citations?user=ygpxbK8AAAAJ">Maja Pantic</a>
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://antonibigata.github.io/KeyFace/">project page</a>
        /
        <a href="https://arxiv.org/abs/2503.01715">arXiv</a>
        <p></p>
        <p>Current facial animation methods struggle with consistency over long durations, leading to unnatural motion and identity drift. We introduce KeyFace, a novel two-stage diffusion-based framework that generates keyframes at low frame rates and interpolates smooth transitions, ensuring natural and coherent animation. Our model captures continuous emotions and non-speech vocalizations (NSVs) like laughter and sighs, setting a new standard for long-form facial animation.</p>
      </td>
    </tr>

    <tr onmouseout="croissant_stop()" onmouseover="croissant_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/croissant_illustr.png' width="160">
        </div>
        <script type="text/javascript">
          function croissant_start() {
            document.getElementById('croissant_image').style.opacity = "1";
          }

          function croissant_stop() {
            document.getElementById('croissant_image').style.opacity = "0";
          }
          croissant_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://huggingface.co/blog/manu/croissant-llm-blog">
          <span class="papertitle">ü•ê CroissantLLM: A Truly Bilingual French-English Language Model</span>
        </a>
        <br>
        <a href="">Manuel Faysse</a>,
        <a href="">Patrick Fernandes</a>,
        <a href="">Nuno M. Guerreiro</a>,
        <a href="">Ant√≥nio Loison</a>,
        <a href="">Duarte M. Alves</a>,
        <a href="">Caio Corro</a>,
        <a href="">Nicolas Boizard</a>,
        <a href="">Jo√£o Alves</a>,
        <a href="">Ricardo Rei</a>,
        <a href="">Pedro H. Martins</a>,
        <strong>Antoni Bigata Casademunt</strong>,
        <a href="">Fran√ßois Yvon</a>,
        <a href="">Andr√© F.T. Martins</a>,
        <a href="">Gautier Viaud</a>,
        <a href="">C√©line Hudelot</a>
        <br>
        <em></em>
        <br>
	<br>
        <em>TMLR</em>, 2025
        <br>
        <a href="https://huggingface.co/blog/manu/croissant-llm-blog">project page</a>
        /
        <a href="https://arxiv.org/abs/2402.00786">arXiv</a>
        <p></p>
        <p>CroissantLLM is a 1.3B bilingual model trained on 3T English and French tokens, designed for high performance on consumer hardware. With a 1:1 English-French training approach, a custom tokenizer, and FrenchBench for evaluation, it sets a new standard for multilingual NLP. Fully open-sourced, it includes datasets, checkpoints, and fine-tuned models.</p>
      </td>
    </tr>


    <tr onmouseout="EP_stop()" onmouseover="EP_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='EP_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/EP_v.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/EP_im.png' width="160">
        </div>
        <script type="text/javascript">
          function EP_start() {
            document.getElementById('EP_image').style.opacity = "1";
          }

          function EP_stop() {
            document.getElementById('EP_image').style.opacity = "0";
          }
          EP_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://neeek2303.github.io/EMOPortraits">
          <span class="papertitle">EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</span>
        </a>
        <br> 
        <a href="">Nikita Drobyshev</a>,
        <strong>Antoni Bigata Casademunt</strong>,
        <a href="">Konstantinos Vougioukas</a>,
        <a href="">Zoe Landgraf</a>,
        <a href="">Stavros Petridis</a>,
        <a href="">Maja Pantic</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://neeek2303.github.io/EMOPortraits">project page</a>
        /
        <a href="https://arxiv.org/abs/2404.19110">arXiv</a>
        <p></p>
        <p> EMOPortraits is a head reenactment model that enhances realism in expressing intense, asymmetric emotions and sets new standards in emotion transfer. Additionally, we integrated a speech-driven mode for improved audio-visual animation and introduced a novel multi-view video dataset that captures a broader range of expressions, addressing a critical gap in existing data.</p>
      </td>
    </tr> 

              

    <tr onmouseout="LM_stop()" onmouseover="LM_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='LM_image'><video  width=100% muted autoplay loop>
          <source src="images/LM.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/LM.png' width=100%>
        </div>
        <script type="text/javascript">
          function LM_start() {
            document.getElementById('LM_image').style.opacity = "1";
          }

          function LM_stop() {
            document.getElementById('LM_image').style.opacity = "0";
          }
          LM_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://sites.google.com/view/laughing-matters/">
          <span class="papertitle">Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models</span>
        </a>
        <br>
          								
		<strong>Antoni Bigata Casademunt</strong>,
		<a href="">Rodrigo Mira</a>,
          <a>Nikita Drobyshev</a>,
		<a href="">Konstantinos Vougioukas</a>,
        <a href="">Stavros Petridis</a>,
        <a href="">Maja Pantic</a>,
		
        <br>
        <em>BMVC</em>, 2023
        <br>
        <a href="https://sites.google.com/view/laughing-matters/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=TuyIp3b4_Jo">video</a>
        /
        <a href="https://arxiv.org/abs/2305.08854">arXiv</a>
        <p></p>
        <p>
          While speech-driven animation has made impressive strides, non-verbal communication‚Äîespecially laughter, remains an open challenge. Our work introduces a novel model that generates realistic laughter sequences from a still portrait and an audio clip. By leveraging diffusion models and training on diverse laughter datasets, we outperform traditional facial animation methods, setting a new benchmark for laughter synthesis. 
        </p>
      </td>
    </tr>
	
<!-- 

    <tr onmouseout="MP_stop()" onmouseover="MP_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='MP_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/MP.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/MP.png' width="160">
        </div>
        <script type="text/javascript">
          function MP_start() {
            document.getElementById('MP_image').style.opacity = "1";
          }

          function MP_stop() {
            document.getElementById('MP_image').style.opacity = "0";
          }
          MP_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://neeek2303.github.io/MegaPortraits/">
          <span class="papertitle">MegaPortraits: One-shot Megapixel Neural Head Avatars
</span>
        </a>
        <br> 
        <strong>Nikita Drobyshev</strong>,
        <a href="">Jenya Chelishev</a>,
        <a href="">Taras Khakhulin</a>,
        <a href="">Aleksei Ivakhnenko</a>,
        <a href="">Viktor Lempitsky</a>,
        <a href="">Egor Zakharov</a>
        <br>
        <em>ACMM</em>, 2022
        <br>
        <a href="https://neeek2303.github.io/MegaPortraits/">project page</a>
        /
        <a href="https://arxiv.org/abs/2207.07621">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=9D5ulvdg0jM">video</a>
        <p></p>
        <p> MegaPortraits advance the neural head avatar technology to the megapixel resolution while focusing on the particularly challenging task of cross-driving synthesis, i.e., when the appearance of the driving image is substantially different from the animated source image.</p>
      </td>
    </tr> 




              
  <tr onmouseout="UDSR_stop()" onmouseover="UDSR_start()">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='UDSR_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/DD.png" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/DD.png' width="160">
      </div>
      <script type="text/javascript">
        function UDSR_start() {
          document.getElementById('UDSR_image').style.opacity = "1";
        }

        function UDSR_stop() {
          document.getElementById('UDSR_image').style.opacity = "0";
        }
        UDSR_stop()
      </script>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2105.12038">
        <span class="papertitle">Unpaired Depth Super-Resolution in the Wild</span>
      </a>
      <br>
      <a href="">Aleksandr Safin*</a>,
      <strong>Nikita Drobyshev*</strong>,
      <a href="">Maxim Kan*</a>,
      <a href="">Oleg Voynov</a>,
      <a href="">Alexey Artemov</a>,
      <a href="">Alexander Filippov</a>,
      <a href="">Denis Zorin</a>,
      <a href="">Evgeny Burnaev</a>,
      <br>
      <em>arXiv</em>, 2021
      <br>
      <a href="https://arxiv.org/abs/2105.12038">arXiv</a>
      <p></p>
      <p>
      We propose an unpaired learning method for depth super-resolution, which is based on a learnable degradation model, enhancement component and surface normal estimates as features to produce more accurate depth maps. 
      </p>
    </td>
  </tr>
	



              
    <tr onmouseout="brain_stop()" onmouseover="brain_stopt()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='brain_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/brain_2.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/brain_2.png' width="160">
        </div>
        <script type="text/javascript">
          function brain_start() {
            document.getElementById('brain_image').style.opacity = "1";
          }

          function brain_stop() {
            document.getElementById('brain_image').style.opacity = "0";
          }
          brain_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2105.12038">
			<span class="papertitle">Interpretation of 3D CNNs for Brain MRI Data Classification
</span>
        </a>
        <br> 
        <a href="">Maxim Kan</a>,
		<a href="">Ruslan Aliev</a>,
        <a href="">Anna Rudenko</a>,
        <strong>Nikita Drobyshev</strong>,
        <a href="">Nikita Petrashen</a>,
        <a href="">Ekaterina Kondrateva</a>,
        <a href="">Maxim Sharaev</a>,
        <a href="">Alexander Bernstein</a>,
        <a href="">Evgeny Burnaev</a>,
        <br>
        <em>AIST</em>, 2020
        <br>
        <a href="https://arxiv.org/abs/2105.12038">arXiv</a>
        <p></p>
        <p>
        We extend the previous findings in gender differences from diffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D CNN interpretation comparing the results of three interpretation methods: Meaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute with the open-source library.
        </p>
      </td>
    </tr> -->



            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
